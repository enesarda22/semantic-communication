Index: semantic_communication/models/transceiver.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch import nn\n\nfrom semantic_communication.models.semantic_decoder import SemanticDecoder\nfrom semantic_communication.models.semantic_encoder import SemanticEncoder\nfrom semantic_communication.utils.channel import Channel\nfrom semantic_communication.utils.general import get_device\n\n\nclass ChannelEncComp(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ChannelEncComp, self).__init__()\n        self.linear = nn.Linear(in_dim, out_dim)\n        self.bn = nn.BatchNorm1d(out_dim)\n        self.prelu = nn.PReLU()\n\n    def forward(self, x):\n        x = self.linear(x).transpose(1, 2)\n        x = self.bn(x).transpose(1, 2)\n\n        out = self.prelu(x)\n        return out\n\n\nclass ChannelEncoder(nn.Module):\n    def __init__(self, nin, nout):\n        super(ChannelEncoder, self).__init__()\n        up_dim = int(np.floor(np.log2(nin) / 2))\n        low_dim = int(np.ceil(np.log2(nout) / 2))\n\n        dims = [nin]\n        for i in range(up_dim - low_dim + 1):\n            dims.append(np.power(4, up_dim - i))\n\n        self.layers = nn.ModuleList(\n            [ChannelEncComp(dims[i], dims[i + 1]) for i in range(len(dims) - 1)]\n        )\n\n        self.linear = nn.Linear(dims[-1], nout)\n\n    def forward(self, x):\n        for l in self.layers:\n            x = l(x)\n        return self.linear(x)\n\n\nclass ChannelDecoder(nn.Module):\n    def __init__(self, nin, nout):\n        super(ChannelDecoder, self).__init__()\n        up_dim = int(np.floor(np.log2(nout) / 2))\n        low_dim = int(np.ceil(np.log2(nin) / 2))\n        dims = [nin]\n        for i in range(up_dim - low_dim + 1):\n            dims.append(np.power(4, low_dim + i))\n\n        self.layers = nn.ModuleList(\n            [ChannelEncComp(dims[i], dims[i + 1]) for i in range(len(dims) - 1)]\n        )\n\n        self.linear = nn.Linear(dims[-1], nout)\n\n    def forward(self, x):\n        for l in self.layers:\n            x = l(x)\n        return self.linear(x)\n\n\nclass TxRelayChannelModel(nn.Module):\n    def __init__(self, nin, n_latent, channel: Channel):\n        super(TxRelayChannelModel, self).__init__()\n\n        self.tx_encoder = ChannelEncoder(nin, n_latent)\n        self.relay_decoder = ChannelDecoder(n_latent, nin)\n        self.channel = channel\n\n    def forward(self, x, d_sr):\n        ch_input = self.tx_encoder(x)\n        ch_output = self.channel(ch_input, d_sr)\n        x_hat = self.relay_decoder(ch_output)\n        return x_hat\n\n\nclass TxRelayRxChannelModel(nn.Module):\n    def __init__(\n        self,\n        nin,\n        n_latent,\n        channel: Channel,\n    ):\n        super(TxRelayRxChannelModel, self).__init__()\n\n        self.tx_encoder = ChannelEncoder(nin, n_latent)\n        self.relay_encoder = ChannelEncoder(nin, n_latent)\n        self.rx_decoder = ChannelDecoder(n_latent * 2, nin * 2)\n        self.channel = channel\n\n    def forward(self, tx_x, rel_x, d_rd, d_sd):\n        rel_ch_input = self.relay_encoder(rel_x)\n        tx_ch_input = self.tx_encoder(tx_x)\n\n        rel_ch_out = self.channel(rel_ch_input, d_rd)\n        tx_ch_out = self.channel(tx_ch_input, d_sd)\n\n        ch_output = torch.cat([rel_ch_out, tx_ch_out], dim=-1)  # concatenate\n        x_hat = self.rx_decoder(ch_output)\n        return x_hat\n\n\nclass Transceiver(nn.Module):  # TODO: find a cooler name\n    def __init__(\n        self,\n        semantic_encoder: SemanticEncoder,\n        relay_semantic_decoder: SemanticDecoder,\n        rx_semantic_decoder: SemanticDecoder,\n        tx_relay_channel_enc_dec: TxRelayChannelModel,\n        tx_relay_rx_channel_enc_dec: TxRelayRxChannelModel,\n        encoder: LabelEncoder,\n    ):\n        super().__init__()\n        self.tx_semantic_encoder = semantic_encoder\n        self.relay = Relay(semantic_encoder, relay_semantic_decoder, encoder)\n        self.rx_semantic_decoder = rx_semantic_decoder\n\n        self.tx_relay_channel_enc_dec = tx_relay_channel_enc_dec\n        self.tx_relay_rx_channel_enc_dec = tx_relay_rx_channel_enc_dec\n\n    def forward(self, w, attention_mask, targets, d_sd, d_sr, d_rd):\n        # transmitter\n        encoder_output = self.tx_semantic_encoder(\n            input_ids=w,\n            attention_mask=attention_mask,\n        )\n\n        # relay\n        relay_input = self.tx_relay_channel_enc_dec(encoder_output[:, :-1, :], d_sr)\n        relay_output = self.relay(relay_input)\n\n        # receiver\n        receiver_input = self.tx_relay_rx_channel_enc_dec(\n            encoder_output[:, 1:, :], relay_output, d_rd, d_sd\n        )\n        receiver_output = self.rx_semantic_decoder(\n            encoder_output=receiver_input,\n            attention_mask=attention_mask[:, 1:],\n            targets=targets,\n        )\n        return receiver_output\n\n\nclass Relay(nn.Module):\n    def __init__(\n        self,\n        semantic_encoder: SemanticEncoder,\n        semantic_decoder: SemanticDecoder,\n        encoder: LabelEncoder,\n    ):\n        super().__init__()\n        self.device = get_device()\n        self.semantic_encoder = semantic_encoder\n        self.semantic_decoder = semantic_decoder\n        self.encoder = encoder\n\n    def forward(self, x):\n        B, T, C = x.shape\n\n        self.semantic_decoder.eval()\n        with torch.no_grad():\n            predicted_ids = self.semantic_decoder.generate(x)\n\n        predicted_ids = self.encoder.inverse_transform(\n            predicted_ids.flatten().to(\"cpu\")\n        ).reshape(B, T)\n        predicted_ids = torch.LongTensor(predicted_ids).to(self.device)\n\n        # ids are repeated to generate the embeddings sequentially\n        predicted_ids = torch.repeat_interleave(predicted_ids, T, dim=0)\n\n        # append [CLS] token\n        cls_padding = torch.full((B * T, 1), 101).to(self.device)\n        predicted_ids = torch.cat(\n            tensors=(cls_padding, predicted_ids),\n            dim=1,\n        )\n\n        # tril mask to generate the embeddings sequentially\n        tril_mask = (\n            torch.tril(\n                torch.ones(T, T + 1, dtype=torch.long),\n                diagonal=1,\n            ).repeat(B, 1)\n        ).to(self.device)\n\n        out = self.semantic_encoder(\n            input_ids=predicted_ids,\n            attention_mask=tril_mask,\n        )\n\n        # use eye mask to select the correct embeddings sequentially\n        eye_mask = (torch.eye(T).repeat(1, B) == 1).to(self.device)\n        out = torch.masked_select(out[:, 1:, :].transpose(-1, 0), eye_mask)\n        out = out.view(B, T, C)\n\n        return out\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/semantic_communication/models/transceiver.py b/semantic_communication/models/transceiver.py
--- a/semantic_communication/models/transceiver.py	(revision 8d31e12856b947cf25934e78ab11277019911171)
+++ b/semantic_communication/models/transceiver.py	(date 1700765501963)
@@ -49,6 +49,8 @@
 class ChannelDecoder(nn.Module):
     def __init__(self, nin, nout):
         super(ChannelDecoder, self).__init__()
+        self.ln = nn.LayerNorm(nin)
+
         up_dim = int(np.floor(np.log2(nout) / 2))
         low_dim = int(np.ceil(np.log2(nin) / 2))
         dims = [nin]
@@ -62,6 +64,7 @@
         self.linear = nn.Linear(dims[-1], nout)
 
     def forward(self, x):
+        x = self.ln(x)
         for l in self.layers:
             x = l(x)
         return self.linear(x)
