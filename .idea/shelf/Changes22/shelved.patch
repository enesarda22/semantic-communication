Index: generate_sequences_src_relay_block.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/generate_sequences_src_relay_block.py b/generate_sequences_src_relay_block.py
new file mode 100644
--- /dev/null	(date 1714336642717)
+++ b/generate_sequences_src_relay_block.py	(date 1714336642717)
@@ -0,0 +1,116 @@
+import argparse
+
+from tqdm import tqdm
+
+from semantic_communication.data_processing.data_handler import DataHandler
+from semantic_communication.models.semantic_decoder import SemanticDecoder
+from semantic_communication.models.semantic_encoder import SemanticEncoder
+from semantic_communication.models.semantic_transformer import SemanticTransformer
+from semantic_communication.models.transceiver import (
+    ChannelDecoder,
+    ChannelEncoder,
+    SrcRelayBlock,
+)
+from semantic_communication.utils.channel import init_channel
+from semantic_communication.utils.general import (
+    add_semantic_decoder_args,
+    add_data_args,
+    add_channel_model_args,
+    set_seed,
+    get_device,
+    load_model,
+)
+
+
+def generate_text():
+    for b in tqdm(data_handler.test_dataloader):
+        encoder_idx = b[0].to(device)
+        encoder_attention_mask = b[1].to(device)
+
+        encoder_idx = data_handler.label_encoder.transform(encoder_idx)
+
+        # d_sd = get_distance(args.d_min, args.d_max)
+        # d_sr = get_distance(d_sd * args.gamma_min, d_sd * args.gamma_max)
+
+        predicted_ids, _ = src_relay_block.generate(
+            input_ids=encoder_idx,
+            attention_mask=encoder_attention_mask,
+            d_sr=1000.0,
+        )
+
+        predicted_tokens = semantic_encoder.get_tokens(
+            ids=predicted_ids,
+            skip_special_tokens=True,
+        )
+
+        input_tokens = semantic_encoder.get_tokens(
+            ids=encoder_idx,
+            skip_special_tokens=True,
+        )
+
+        for i in range(len(input_tokens)):
+            print(f"\nTrue Sentence: {input_tokens[i]}")
+            for k in range(args.rate):
+                print(f"Predicted Sentence {k}: {predicted_tokens[i * args.rate + k]}")
+
+
+if __name__ == "__main__":
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--src-relay-block-path", type=str)
+    parser.add_argument("--batch-size", default=32, type=int)
+    add_semantic_decoder_args(parser)
+    add_data_args(parser)
+    add_channel_model_args(parser)
+    args = parser.parse_args()
+
+    set_seed()
+    device = get_device()
+
+    data_handler = DataHandler(
+        batch_size=args.batch_size,
+        data_fp=args.data_fp,
+    )
+
+    semantic_encoder = SemanticEncoder(
+        label_encoder=data_handler.label_encoder,
+        max_length=args.max_length,
+        mode=args.mode,
+        rate=args.rate,
+    ).to(device)
+
+    semantic_decoder = SemanticDecoder(
+        vocab_size=data_handler.vocab_size,
+        n_blocks=args.n_blocks,
+        n_heads=args.n_heads,
+        n_embeddings=args.n_embeddings,
+        block_size=args.max_length,
+        bert=semantic_encoder.bert,
+        pad_idx=data_handler.label_encoder.pad_id,
+    ).to(device)
+
+    semantic_transformer = SemanticTransformer(
+        semantic_encoder=semantic_encoder,
+        semantic_decoder=semantic_decoder,
+    ).to(device)
+
+    src_channel_encoder = ChannelEncoder(
+        nin=args.channel_block_input_dim,
+        nout=args.channel_block_latent_dim,
+    ).to(device)
+
+    relay_channel_decoder = ChannelDecoder(
+        nin=args.channel_block_latent_dim,
+        nout=args.channel_block_input_dim,
+    ).to(device)
+
+    channel = init_channel(args.channel_type, args.sig_pow, args.alpha, args.noise_pow)
+
+    src_relay_block = SrcRelayBlock(
+        semantic_transformer=semantic_transformer,
+        src_channel_encoder=src_channel_encoder,
+        relay_channel_decoder=relay_channel_decoder,
+        channel=channel,
+    ).to(device)
+    load_model(src_relay_block, args.src_relay_block_path)
+
+    generate_text()
Index: generate_sequences_semantic_transformer.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import argparse\n\nimport torch\nfrom tqdm import tqdm\n\nfrom semantic_communication.data_processing.data_handler import DataHandler\nfrom semantic_communication.models.semantic_decoder import SemanticDecoder\nfrom semantic_communication.models.semantic_encoder import SemanticEncoder\nfrom semantic_communication.models.semantic_transformer import SemanticTransformer\nfrom semantic_communication.utils.general import (\n    add_semantic_decoder_args,\n    add_data_args,\n    set_seed,\n    get_device,\n    load_model,\n)\n\n\ndef generate_text():\n    for b in tqdm(data_handler.test_dataloader):\n        encoder_idx = b[0].to(device)\n        encoder_attention_mask = b[1].to(device)\n\n        encoder_idx = data_handler.label_encoder.transform(encoder_idx)\n\n        predicted_ids, probs = semantic_transformer.generate(\n            input_ids=encoder_idx,\n            attention_mask=encoder_attention_mask,\n            max_length=args.max_length,\n            snr_db=args.snr_db,\n        )\n\n        predicted_tokens = semantic_encoder.get_tokens(\n            ids=predicted_ids,\n            skip_special_tokens=True,\n        )\n\n        input_tokens = semantic_encoder.get_tokens(\n            ids=encoder_idx,\n            skip_special_tokens=True,\n        )\n\n        _, indices = torch.sort(probs)\n        for i in indices:\n            print(f\"Probability: {torch.exp(probs[i]):.2e}\")\n            print(f\"{input_tokens[i]}\\n{predicted_tokens[i]}\\n\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--semantic-transformer-path\", type=str)\n    parser.add_argument(\"--batch-size\", default=32, type=int)\n    parser.add_argument(\"--snr-db\", default=None, type=float)\n    add_semantic_decoder_args(parser)\n    add_data_args(parser)\n    args = parser.parse_args()\n\n    set_seed()\n    device = get_device()\n\n    data_handler = DataHandler(\n        batch_size=args.batch_size,\n        data_fp=args.data_fp,\n    )\n\n    semantic_encoder = SemanticEncoder(\n        label_encoder=data_handler.label_encoder,\n        max_length=args.max_length,\n        mode=args.mode,\n    ).to(device)\n\n    semantic_decoder = SemanticDecoder(\n        vocab_size=data_handler.vocab_size,\n        n_blocks=args.n_blocks,\n        n_heads=args.n_heads,\n        n_embeddings=args.n_embeddings,\n        block_size=args.max_length,\n        bert=semantic_encoder.bert,\n    ).to(device)\n\n    semantic_transformer = SemanticTransformer(\n        semantic_encoder=semantic_encoder,\n        semantic_decoder=semantic_decoder,\n    ).to(device)\n    load_model(semantic_transformer, args.semantic_transformer_path)\n\n    generate_text()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/generate_sequences_semantic_transformer.py b/generate_sequences_semantic_transformer.py
--- a/generate_sequences_semantic_transformer.py	(revision cad0f871753e4e48d32315ebcb817d0c732a67c2)
+++ b/generate_sequences_semantic_transformer.py	(date 1714336642715)
@@ -26,8 +26,9 @@
         predicted_ids, probs = semantic_transformer.generate(
             input_ids=encoder_idx,
             attention_mask=encoder_attention_mask,
-            max_length=args.max_length,
             snr_db=args.snr_db,
+            max_length=args.max_length,
+            n_generated_tokens=args.max_length + 1,
         )
 
         predicted_tokens = semantic_encoder.get_tokens(
@@ -67,6 +68,7 @@
         label_encoder=data_handler.label_encoder,
         max_length=args.max_length,
         mode=args.mode,
+        rate=args.rate,
     ).to(device)
 
     semantic_decoder = SemanticDecoder(
@@ -76,6 +78,7 @@
         n_embeddings=args.n_embeddings,
         block_size=args.max_length,
         bert=semantic_encoder.bert,
+        pad_idx=data_handler.label_encoder.pad_id,
     ).to(device)
 
     semantic_transformer = SemanticTransformer(
Index: eval.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import argparse\nimport numpy as np\n\nfrom nltk.translate.bleu_score import sentence_bleu\n\nimport torch\nfrom torch.nn import functional as F\n\nfrom semantic_communication.models.transceiver import (\n    TxRelayChannelModel,\n    TxRelayRxChannelModel,\n    Transceiver,\n    ChannelEncoder,\n    RelayChannelBlock,\n)\nfrom semantic_communication.utils.general import (\n    get_device,\n    set_seed,\n    add_semantic_decoder_args,\n    add_channel_model_args,\n    add_data_args,\n)\nfrom semantic_communication.models.semantic_encoder import SemanticEncoder\nfrom semantic_communication.data_processing.data_handler import DataHandler\nfrom semantic_communication.models.semantic_decoder import SemanticDecoder\nfrom semantic_communication.utils.channel import init_channel\n\n\ndef semantic_similarity_score(target_sentences, received_sentences):\n    target_emb = semantic_encoder(messages=target_sentences)\n    received_emb = semantic_encoder(messages=received_sentences)\n    scores = F.cosine_similarity(target_emb, received_emb)\n\n    return scores\n\n\ndef bleu_1gram(target_sentences, received_sentences):\n    return sentence_bleu([target_sentences], received_sentences, weights=(1, 0, 0, 0))\n\n\ndef bleu_2gram(target_sentences, received_sentences):\n    return sentence_bleu([target_sentences], received_sentences, weights=(0, 1, 0, 0))\n\n\ndef bleu_3gram(target_sentences, received_sentences):\n    return sentence_bleu([target_sentences], received_sentences, weights=(0, 0, 1, 0))\n\n\ndef bleu_4gram(target_sentences, received_sentences):\n    return sentence_bleu([target_sentences], received_sentences, weights=(0, 0, 0, 1))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    # model args\n    parser.add_argument(\"--transceiver-path\", type=str)\n    add_semantic_decoder_args(parser)\n    add_channel_model_args(parser)\n    add_data_args(parser)\n\n    # test args\n    parser.add_argument(\"--batch-size\", default=128, type=int)\n    parser.add_argument(\"--gamma-list\", nargs=\"+\", type=float)\n    parser.add_argument(\"--d-list\", nargs=\"+\", type=float)\n    parser.add_argument(\"--n-test\", default=10000, type=int)\n    parser.add_argument(\"--semantic-similarity-threshold\", default=0.8, type=float)\n    parser.add_argument(\"--bleu-1-threshold\", default=0.5, type=float)\n    parser.add_argument(\"--bleu-3-threshold\", default=0.5, type=float)\n\n    args = parser.parse_args()\n    device = get_device()\n    set_seed()\n\n    semantic_encoder = SemanticEncoder(max_length=args.max_length)\n    data_handler = DataHandler(\n        semantic_encoder=semantic_encoder,\n        data_fp=args.data_fp,\n        batch_size=args.batch_size,\n    )\n\n    # initialize models\n    relay_decoder = SemanticDecoder(\n        vocab_size=data_handler.vocab_size,\n        n_blocks=args.n_blocks,\n        n_heads=args.n_heads,\n        n_embeddings=args.n_embeddings,\n        block_size=args.max_length,\n    ).to(device)\n\n    tx_channel_enc = ChannelEncoder(\n        nin=args.channel_block_input_dim,\n        nout=args.channel_block_latent_dim,\n    ).to(device)\n\n    channel = init_channel(args.channel_type, args.sig_pow, args.alpha, args.noise_pow)\n    tx_relay_channel_model = TxRelayChannelModel(\n        nin=args.channel_block_input_dim,\n        n_latent=args.channel_block_latent_dim,\n        channel=channel,\n    ).to(device)\n\n    relay_channel_block = RelayChannelBlock(\n        semantic_decoder=relay_decoder,\n        tx_channel_enc=tx_channel_enc,\n        tx_relay_channel_enc_dec=tx_relay_channel_model,\n    ).to(device)\n\n    receiver_decoder = SemanticDecoder(\n        vocab_size=data_handler.vocab_size,\n        n_blocks=args.n_blocks,\n        n_heads=args.n_heads,\n        n_embeddings=args.n_embeddings * 2,\n        block_size=args.max_length,\n    ).to(device)\n\n    tx_relay_rx_channel_model = TxRelayRxChannelModel(\n        nin=args.channel_block_input_dim,\n        n_latent=args.channel_block_latent_dim,\n        channel=channel,\n    ).to(device)\n\n    transceiver = Transceiver(\n        semantic_encoder=semantic_encoder,\n        relay_channel_block=relay_channel_block,\n        rx_semantic_decoder=receiver_decoder,\n        tx_relay_rx_channel_enc_dec=tx_relay_rx_channel_model,\n        encoder=data_handler.encoder,\n    )\n    transceiver_checkpoint = torch.load(args.transceiver_path, map_location=device)\n    transceiver.load_state_dict(transceiver_checkpoint[\"model_state_dict\"])\n\n    mean_semantic_sim = np.zeros((len(args.d_list), len(args.gamma_list)))\n    mean_bleu_1 = np.zeros((len(args.d_list), len(args.gamma_list)))\n    mean_bleu_3 = np.zeros((len(args.d_list), len(args.gamma_list)))\n\n    std_semantic_sim = np.zeros((len(args.d_list), len(args.gamma_list)))\n    std_bleu_1 = np.zeros((len(args.d_list), len(args.gamma_list)))\n    std_bleu_3 = np.zeros((len(args.d_list), len(args.gamma_list)))\n\n    semantic_sim_efficiency = np.zeros((len(args.d_list), len(args.gamma_list)))\n    bleu_1_efficiency = np.zeros((len(args.d_list), len(args.gamma_list)))\n    bleu_3_efficiency = np.zeros((len(args.d_list), len(args.gamma_list)))\n\n    semantic_sim_efficiency_se = np.zeros((len(args.d_list), len(args.gamma_list)))\n    bleu_1_efficiency_se = np.zeros((len(args.d_list), len(args.gamma_list)))\n    bleu_3_efficiency_se = np.zeros((len(args.d_list), len(args.gamma_list)))\n\n    # For each d_sd\n    for distance_index, d_sd in enumerate(args.d_list):\n        # For each gamma in gamma list\n        for gamma_index, gamma in enumerate(args.gamma_list):\n            print(f\"Simulating for distance: {d_sd}  - Gamma: {gamma}\")\n\n            cosine_scores = []\n            bleu1_scores = []\n            bleu3_scores = []\n\n            d_sr = d_sd * gamma\n            d_rd = d_sd - d_sr\n\n            transceiver.eval()\n\n            time_slot = 0\n            semantic_similarity_num_correct_sentences = 0\n            bleu_1_num_correct_sentences = 0\n            bleu_3_num_correct_sentences = 0\n\n            for b in data_handler.test_dataloader:\n                xb = b[0].to(device)\n                targets = data_handler.encode_token_ids(xb)\n                attention_mask = b[1].to(device)\n                time_slot += (\n                    torch.sum(attention_mask) - attention_mask.shape[0]\n                ).item()\n\n                B, T = xb.shape\n                with torch.no_grad():\n                    logits, _ = transceiver(\n                        xb, attention_mask, targets[:, 1:], d_sd, d_sr, d_rd\n                    )\n                    probs = F.softmax(logits, dim=-1)\n                    predicted_ids = (torch.argmax(probs, dim=-1)).reshape(\n                        B, args.max_length\n                    )\n\n                    end_token_id = data_handler.encoder.transform([102])[0]\n                    end_prediction_idx = torch.argmax(\n                        predicted_ids.eq(end_token_id).double(), dim=1\n                    )\n\n                    # zero means no end token prediction\n                    end_prediction_idx[end_prediction_idx == 0] = T - 1\n\n                    # prediction mask is created based on end token predictions\n                    pred_mask = (torch.arange(T - 1).to(device)).le(\n                        end_prediction_idx.view(-1, 1)\n                    )\n\n                    predicted_sentences = data_handler.get_tokens(\n                        ids=predicted_ids,\n                        attention_mask=pred_mask,\n                        skip_special_tokens=True,\n                    )\n\n                    original_sentences = data_handler.get_tokens(\n                        ids=targets,\n                        attention_mask=attention_mask,\n                        skip_special_tokens=True,\n                    )\n\n                    for s1, s2 in zip(original_sentences, predicted_sentences):\n                        cosine_score = semantic_similarity_score([s1], [s2])[0][\n                            0\n                        ].item()\n                        bleu1_score = bleu_1gram(s1, s2)\n                        bleu3_score = bleu_3gram(s1, s2)\n\n                        if args.semantic_similarity_threshold <= cosine_score:\n                            semantic_similarity_num_correct_sentences += 1\n\n                        if args.bleu_1_threshold <= bleu1_score:\n                            bleu_1_num_correct_sentences += 1\n\n                        if args.bleu_3_threshold <= bleu3_score:\n                            bleu_3_num_correct_sentences += 1\n\n                        cosine_scores.append(cosine_score)\n                        bleu1_scores.append(bleu1_score)\n                        bleu3_scores.append(bleu3_score)\n                if len(cosine_scores) > args.n_test:\n                    break\n\n            n_test_samples = len(cosine_scores)\n\n            time_slot = time_slot + n_test_samples\n\n            semantic_sim_efficiency[distance_index, gamma_index] = (\n                semantic_similarity_num_correct_sentences / time_slot\n            )\n            bleu_1_efficiency[distance_index, gamma_index] = (\n                bleu_1_num_correct_sentences / time_slot\n            )\n            bleu_3_efficiency[distance_index, gamma_index] = (\n                bleu_3_num_correct_sentences / time_slot\n            )\n\n            semantic_sim_efficiency_se[distance_index, gamma_index] = (\n                (n_test_samples**0.5) / time_slot\n            ) * (\n                semantic_similarity_num_correct_sentences\n                * (1 - semantic_similarity_num_correct_sentences / n_test_samples) ** 2\n                + (n_test_samples - semantic_similarity_num_correct_sentences)\n                * (semantic_similarity_num_correct_sentences / n_test_samples) ** 2\n            ) ** 0.5\n\n            bleu_1_efficiency_se[distance_index, gamma_index] = (\n                (n_test_samples**0.5) / time_slot\n            ) * (\n                bleu_1_num_correct_sentences\n                * (1 - bleu_1_num_correct_sentences / n_test_samples) ** 2\n                + (n_test_samples - bleu_1_num_correct_sentences)\n                * (bleu_1_num_correct_sentences / n_test_samples) ** 2\n            ) ** 0.5\n\n            bleu_3_efficiency_se[distance_index, gamma_index] = (\n                (n_test_samples**0.5) / time_slot\n            ) * (\n                bleu_3_num_correct_sentences\n                * (1 - bleu_3_num_correct_sentences / n_test_samples) ** 2\n                + (n_test_samples - bleu_3_num_correct_sentences)\n                * (bleu_3_num_correct_sentences / n_test_samples) ** 2\n            ) ** 0.5\n\n            mean_semantic_sim[distance_index, gamma_index] = np.mean(cosine_scores)\n            mean_bleu_1[distance_index, gamma_index] = np.mean(bleu1_scores)\n            mean_bleu_3[distance_index, gamma_index] = np.mean(bleu3_scores)\n\n            std_semantic_sim[distance_index, gamma_index] = np.std(\n                cosine_scores, ddof=1\n            ) / np.sqrt(n_test_samples)\n            std_bleu_1[distance_index, gamma_index] = np.std(\n                bleu1_scores, ddof=1\n            ) / np.sqrt(n_test_samples)\n            std_bleu_3[distance_index, gamma_index] = np.std(\n                bleu3_scores, ddof=1\n            ) / np.sqrt(n_test_samples)\n\n    np.save(\"spf_mean_semantic_sim.npy\", mean_semantic_sim)\n    np.save(\"spf_mean_bleu_1.npy\", mean_bleu_1)\n    np.save(\"spf_mean_bleu_3.npy\", mean_bleu_3)\n\n    np.save(\"spf_std_semantic_sim.npy\", std_semantic_sim)\n    np.save(\"spf_std_bleu_1.npy\", std_bleu_1)\n    np.save(\"spf_std_bleu_3.npy\", std_bleu_3)\n\n    np.save(\"spf_efficiency_semantic_sim.npy\", semantic_sim_efficiency)\n    np.save(\"spf_efficiency_bleu_1.npy\", bleu_1_efficiency)\n    np.save(\"spf_efficiency_bleu_3.npy\", bleu_3_efficiency)\n\n    np.save(\"spf_efficiency_semantic_sim_se.npy\", semantic_sim_efficiency_se)\n    np.save(\"spf_efficiency_bleu_1_se.npy\", bleu_1_efficiency_se)\n    np.save(\"spf_efficiency_bleu_3_se.npy\", bleu_3_efficiency_se)\n\n    # d_sr_np = np.array(args.gamma_list) * args.d\n    #\n    # plt.figure()\n    # plt.plot(d_sr_np, mean_semantic_sim)\n    # plt.grid()\n    # plt.xlabel(\"S-R Distance\")\n    # plt.ylabel(\"Semantic Similarity\")\n    # plt.title(\"Semantic Similarity v. S-R Distance Ratio\")\n    # plt.savefig(\"SemanticSimilarty_v_distance.png\", dpi=400)\n    #\n    # plt.figure()\n    # plt.plot(d_sr_np, mean_bleu_1)\n    # plt.grid()\n    # plt.xlabel(\"S-R Distance\")\n    # plt.ylabel(\"BLEU 1-gram\")\n    # plt.title(\"BLEU 1-gram v. S-R Distance\")\n    # plt.savefig(\"BLEU1gram_v_distance.png\", dpi=400)\n    #\n    # plt.figure()\n    # plt.plot(d_sr_np, mean_bleu_2)\n    # plt.grid()\n    # plt.xlabel(\"S-R Distance\")\n    # plt.ylabel(\"BLEU 2-gram\")\n    # plt.title(\"BLEU 2-gram v. S-R Distance\")\n    # plt.savefig(\"BLEU2gam_v_distance.png\", dpi=400)\n    #\n    # plt.figure()\n    # plt.plot(d_sr_np, mean_bleu_3)\n    # plt.grid()\n    # plt.xlabel(\"S-R Distance\")\n    # plt.ylabel(\"BLEU 3-gram\")\n    # plt.title(\"BLEU 3-gram v. S-R Distance\")\n    # plt.savefig(\"BLEU3gram_v_distance.png\", dpi=400)\n    #\n    # plt.figure()\n    # plt.plot(d_sr_np, mean_bleu_4)\n    # plt.grid()\n    # plt.xlabel(\"S-R Distance\")\n    # plt.ylabel(\"BLEU 4-gram\")\n    # plt.title(\"BLEU 4-gram v. S-R Distance\")\n    # plt.savefig(\"BLEU4gram_v_distance.png\", dpi=400)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/eval.py b/eval.py
--- a/eval.py	(revision cad0f871753e4e48d32315ebcb817d0c732a67c2)
+++ b/eval.py	(date 1714336642714)
@@ -2,16 +2,14 @@
 import numpy as np
 
 from nltk.translate.bleu_score import sentence_bleu
+from torch import nn
 
-import torch
-from torch.nn import functional as F
-
+from semantic_communication.models.semantic_transformer import SemanticTransformer
 from semantic_communication.models.transceiver import (
-    TxRelayChannelModel,
-    TxRelayRxChannelModel,
     Transceiver,
     ChannelEncoder,
-    RelayChannelBlock,
+    ChannelDecoder,
+    SrcRelayBlock,
 )
 from semantic_communication.utils.general import (
     get_device,
@@ -19,6 +17,7 @@
     add_semantic_decoder_args,
     add_channel_model_args,
     add_data_args,
+    load_model,
 )
 from semantic_communication.models.semantic_encoder import SemanticEncoder
 from semantic_communication.data_processing.data_handler import DataHandler
@@ -26,14 +25,6 @@
 from semantic_communication.utils.channel import init_channel
 
 
-def semantic_similarity_score(target_sentences, received_sentences):
-    target_emb = semantic_encoder(messages=target_sentences)
-    received_emb = semantic_encoder(messages=received_sentences)
-    scores = F.cosine_similarity(target_emb, received_emb)
-
-    return scores
-
-
 def bleu_1gram(target_sentences, received_sentences):
     return sentence_bleu([target_sentences], received_sentences, weights=(1, 0, 0, 0))
 
@@ -64,91 +55,110 @@
     parser.add_argument("--gamma-list", nargs="+", type=float)
     parser.add_argument("--d-list", nargs="+", type=float)
     parser.add_argument("--n-test", default=10000, type=int)
-    parser.add_argument("--semantic-similarity-threshold", default=0.8, type=float)
-    parser.add_argument("--bleu-1-threshold", default=0.5, type=float)
-    parser.add_argument("--bleu-3-threshold", default=0.5, type=float)
 
     args = parser.parse_args()
     device = get_device()
     set_seed()
 
-    semantic_encoder = SemanticEncoder(max_length=args.max_length)
     data_handler = DataHandler(
-        semantic_encoder=semantic_encoder,
+        batch_size=args.batch_size,
         data_fp=args.data_fp,
-        batch_size=args.batch_size,
     )
 
     # initialize models
-    relay_decoder = SemanticDecoder(
+    semantic_encoder = SemanticEncoder(
+        label_encoder=data_handler.label_encoder,
+        max_length=args.max_length,
+        mode=args.mode,
+        rate=args.rate,
+    ).to(device)
+
+    semantic_decoder = SemanticDecoder(
         vocab_size=data_handler.vocab_size,
         n_blocks=args.n_blocks,
         n_heads=args.n_heads,
         n_embeddings=args.n_embeddings,
         block_size=args.max_length,
+        bert=semantic_encoder.bert,
+        pad_idx=data_handler.label_encoder.pad_id,
     ).to(device)
 
-    tx_channel_enc = ChannelEncoder(
+    semantic_transformer = SemanticTransformer(
+        semantic_encoder=semantic_encoder,
+        semantic_decoder=semantic_decoder,
+    ).to(device)
+
+    src_channel_encoder = ChannelEncoder(
         nin=args.channel_block_input_dim,
         nout=args.channel_block_latent_dim,
     ).to(device)
+
+    relay_channel_decoder = ChannelDecoder(
+        nin=args.channel_block_latent_dim,
+        nout=args.channel_block_input_dim,
+    ).to(device)
 
     channel = init_channel(args.channel_type, args.sig_pow, args.alpha, args.noise_pow)
-    tx_relay_channel_model = TxRelayChannelModel(
-        nin=args.channel_block_input_dim,
-        n_latent=args.channel_block_latent_dim,
+
+    src_relay_block = SrcRelayBlock(
+        semantic_transformer=semantic_transformer,
+        src_channel_encoder=src_channel_encoder,
+        relay_channel_decoder=relay_channel_decoder,
         channel=channel,
     ).to(device)
 
-    relay_channel_block = RelayChannelBlock(
-        semantic_decoder=relay_decoder,
-        tx_channel_enc=tx_channel_enc,
-        tx_relay_channel_enc_dec=tx_relay_channel_model,
+    relay_semantic_encoder = SemanticEncoder(
+        label_encoder=data_handler.label_encoder,
+        max_length=args.max_length,
+        mode=args.mode if args.mode == "sentence" else "forward",
+        rate=args.rate,
+    ).to(device)
+
+    relay_channel_encoder = ChannelEncoder(
+        nin=args.channel_block_input_dim,
+        nout=args.channel_block_latent_dim,
     ).to(device)
 
-    receiver_decoder = SemanticDecoder(
+    dst_channel_decoder = ChannelDecoder(
+        nin=args.channel_block_latent_dim * 2,
+        nout=args.channel_block_input_dim,
+    ).to(device)
+
+    dst_semantic_decoder = SemanticDecoder(
         vocab_size=data_handler.vocab_size,
         n_blocks=args.n_blocks,
         n_heads=args.n_heads,
-        n_embeddings=args.n_embeddings * 2,
+        n_embeddings=args.n_embeddings,
         block_size=args.max_length,
+        bert=semantic_encoder.bert,
+        pad_idx=data_handler.label_encoder.pad_id,
     ).to(device)
 
-    tx_relay_rx_channel_model = TxRelayRxChannelModel(
-        nin=args.channel_block_input_dim,
-        n_latent=args.channel_block_latent_dim,
+    transceiver = Transceiver(
+        src_relay_block=src_relay_block,
+        relay_semantic_encoder=relay_semantic_encoder,
+        relay_channel_encoder=relay_channel_encoder,
+        dst_channel_decoder=dst_channel_decoder,
+        dst_semantic_decoder=dst_semantic_decoder,
         channel=channel,
-    ).to(device)
-
-    transceiver = Transceiver(
-        semantic_encoder=semantic_encoder,
-        relay_channel_block=relay_channel_block,
-        rx_semantic_decoder=receiver_decoder,
-        tx_relay_rx_channel_enc_dec=tx_relay_rx_channel_model,
-        encoder=data_handler.encoder,
+        max_length=args.max_length,
     )
-    transceiver_checkpoint = torch.load(args.transceiver_path, map_location=device)
-    transceiver.load_state_dict(transceiver_checkpoint["model_state_dict"])
+    transceiver = nn.DataParallel(transceiver)  # TODO: remove module. prefix?
+    transceiver = transceiver.to(device)
+    load_model(transceiver, args.transceiver_path)
 
-    mean_semantic_sim = np.zeros((len(args.d_list), len(args.gamma_list)))
-    mean_bleu_1 = np.zeros((len(args.d_list), len(args.gamma_list)))
-    mean_bleu_3 = np.zeros((len(args.d_list), len(args.gamma_list)))
+    n_d = len(args.d_list)
+    n_gamma = len(args.gamma_list)
 
-    std_semantic_sim = np.zeros((len(args.d_list), len(args.gamma_list)))
-    std_bleu_1 = np.zeros((len(args.d_list), len(args.gamma_list)))
-    std_bleu_3 = np.zeros((len(args.d_list), len(args.gamma_list)))
+    mean_bleu_1 = np.zeros((n_d, n_gamma))
+    mean_bleu_3 = np.zeros((n_d, n_gamma))
 
-    semantic_sim_efficiency = np.zeros((len(args.d_list), len(args.gamma_list)))
-    bleu_1_efficiency = np.zeros((len(args.d_list), len(args.gamma_list)))
-    bleu_3_efficiency = np.zeros((len(args.d_list), len(args.gamma_list)))
+    std_bleu_1 = np.zeros((n_d, n_gamma))
+    std_bleu_3 = np.zeros((n_d, n_gamma))
 
-    semantic_sim_efficiency_se = np.zeros((len(args.d_list), len(args.gamma_list)))
-    bleu_1_efficiency_se = np.zeros((len(args.d_list), len(args.gamma_list)))
-    bleu_3_efficiency_se = np.zeros((len(args.d_list), len(args.gamma_list)))
-
-    # For each d_sd
+    # for each d_sd
     for distance_index, d_sd in enumerate(args.d_list):
-        # For each gamma in gamma list
+        # for each gamma in gamma list
         for gamma_index, gamma in enumerate(args.gamma_list):
             print(f"Simulating for distance: {d_sd}  - Gamma: {gamma}")
 
@@ -157,189 +167,46 @@
             bleu3_scores = []
 
             d_sr = d_sd * gamma
-            d_rd = d_sd - d_sr
-
-            transceiver.eval()
-
-            time_slot = 0
-            semantic_similarity_num_correct_sentences = 0
-            bleu_1_num_correct_sentences = 0
-            bleu_3_num_correct_sentences = 0
 
             for b in data_handler.test_dataloader:
-                xb = b[0].to(device)
-                targets = data_handler.encode_token_ids(xb)
-                attention_mask = b[1].to(device)
-                time_slot += (
-                    torch.sum(attention_mask) - attention_mask.shape[0]
-                ).item()
+                encoder_idx = b[0].to(device)
+                encoder_attention_mask = b[1].to(device)
 
-                B, T = xb.shape
-                with torch.no_grad():
-                    logits, _ = transceiver(
-                        xb, attention_mask, targets[:, 1:], d_sd, d_sr, d_rd
-                    )
-                    probs = F.softmax(logits, dim=-1)
-                    predicted_ids = (torch.argmax(probs, dim=-1)).reshape(
-                        B, args.max_length
-                    )
+                encoder_idx = data_handler.label_encoder.transform(encoder_idx)
 
-                    end_token_id = data_handler.encoder.transform([102])[0]
-                    end_prediction_idx = torch.argmax(
-                        predicted_ids.eq(end_token_id).double(), dim=1
-                    )
+                predicted_ids, probs = transceiver.module.generate(
+                    input_ids=encoder_idx,
+                    attention_mask=encoder_attention_mask,
+                    d_sd=d_sd,
+                    d_sr=d_sr,
+                )
 
-                    # zero means no end token prediction
-                    end_prediction_idx[end_prediction_idx == 0] = T - 1
-
-                    # prediction mask is created based on end token predictions
-                    pred_mask = (torch.arange(T - 1).to(device)).le(
-                        end_prediction_idx.view(-1, 1)
-                    )
-
-                    predicted_sentences = data_handler.get_tokens(
-                        ids=predicted_ids,
-                        attention_mask=pred_mask,
-                        skip_special_tokens=True,
-                    )
+                predicted_tokens = semantic_encoder.get_tokens(
+                    ids=predicted_ids,
+                    skip_special_tokens=True,
+                )
 
-                    original_sentences = data_handler.get_tokens(
-                        ids=targets,
-                        attention_mask=attention_mask,
-                        skip_special_tokens=True,
-                    )
+                input_tokens = semantic_encoder.get_tokens(
+                    ids=encoder_idx,
+                    skip_special_tokens=True,
+                )
 
-                    for s1, s2 in zip(original_sentences, predicted_sentences):
-                        cosine_score = semantic_similarity_score([s1], [s2])[0][
-                            0
-                        ].item()
-                        bleu1_score = bleu_1gram(s1, s2)
-                        bleu3_score = bleu_3gram(s1, s2)
+                for s1, s2 in zip(input_tokens, predicted_tokens):
+                    print(f"True Sentence: {s1}\nPredicted Sentence: {s2}\n")
+
+                    bleu1_score = bleu_1gram(s1, s2)
+                    bleu3_score = bleu_3gram(s1, s2)
 
-                        if args.semantic_similarity_threshold <= cosine_score:
-                            semantic_similarity_num_correct_sentences += 1
-
-                        if args.bleu_1_threshold <= bleu1_score:
-                            bleu_1_num_correct_sentences += 1
-
-                        if args.bleu_3_threshold <= bleu3_score:
-                            bleu_3_num_correct_sentences += 1
-
-                        cosine_scores.append(cosine_score)
-                        bleu1_scores.append(bleu1_score)
-                        bleu3_scores.append(bleu3_score)
+                    bleu1_scores.append(bleu1_score)
+                    bleu3_scores.append(bleu3_score)
+
                 if len(cosine_scores) > args.n_test:
                     break
 
             n_test_samples = len(cosine_scores)
 
-            time_slot = time_slot + n_test_samples
-
-            semantic_sim_efficiency[distance_index, gamma_index] = (
-                semantic_similarity_num_correct_sentences / time_slot
-            )
-            bleu_1_efficiency[distance_index, gamma_index] = (
-                bleu_1_num_correct_sentences / time_slot
-            )
-            bleu_3_efficiency[distance_index, gamma_index] = (
-                bleu_3_num_correct_sentences / time_slot
-            )
-
-            semantic_sim_efficiency_se[distance_index, gamma_index] = (
-                (n_test_samples**0.5) / time_slot
-            ) * (
-                semantic_similarity_num_correct_sentences
-                * (1 - semantic_similarity_num_correct_sentences / n_test_samples) ** 2
-                + (n_test_samples - semantic_similarity_num_correct_sentences)
-                * (semantic_similarity_num_correct_sentences / n_test_samples) ** 2
-            ) ** 0.5
-
-            bleu_1_efficiency_se[distance_index, gamma_index] = (
-                (n_test_samples**0.5) / time_slot
-            ) * (
-                bleu_1_num_correct_sentences
-                * (1 - bleu_1_num_correct_sentences / n_test_samples) ** 2
-                + (n_test_samples - bleu_1_num_correct_sentences)
-                * (bleu_1_num_correct_sentences / n_test_samples) ** 2
-            ) ** 0.5
-
-            bleu_3_efficiency_se[distance_index, gamma_index] = (
-                (n_test_samples**0.5) / time_slot
-            ) * (
-                bleu_3_num_correct_sentences
-                * (1 - bleu_3_num_correct_sentences / n_test_samples) ** 2
-                + (n_test_samples - bleu_3_num_correct_sentences)
-                * (bleu_3_num_correct_sentences / n_test_samples) ** 2
-            ) ** 0.5
-
-            mean_semantic_sim[distance_index, gamma_index] = np.mean(cosine_scores)
-            mean_bleu_1[distance_index, gamma_index] = np.mean(bleu1_scores)
-            mean_bleu_3[distance_index, gamma_index] = np.mean(bleu3_scores)
-
-            std_semantic_sim[distance_index, gamma_index] = np.std(
-                cosine_scores, ddof=1
-            ) / np.sqrt(n_test_samples)
-            std_bleu_1[distance_index, gamma_index] = np.std(
-                bleu1_scores, ddof=1
-            ) / np.sqrt(n_test_samples)
-            std_bleu_3[distance_index, gamma_index] = np.std(
-                bleu3_scores, ddof=1
-            ) / np.sqrt(n_test_samples)
-
-    np.save("spf_mean_semantic_sim.npy", mean_semantic_sim)
     np.save("spf_mean_bleu_1.npy", mean_bleu_1)
     np.save("spf_mean_bleu_3.npy", mean_bleu_3)
 
-    np.save("spf_std_semantic_sim.npy", std_semantic_sim)
     np.save("spf_std_bleu_1.npy", std_bleu_1)
     np.save("spf_std_bleu_3.npy", std_bleu_3)
-
-    np.save("spf_efficiency_semantic_sim.npy", semantic_sim_efficiency)
-    np.save("spf_efficiency_bleu_1.npy", bleu_1_efficiency)
-    np.save("spf_efficiency_bleu_3.npy", bleu_3_efficiency)
-
-    np.save("spf_efficiency_semantic_sim_se.npy", semantic_sim_efficiency_se)
-    np.save("spf_efficiency_bleu_1_se.npy", bleu_1_efficiency_se)
-    np.save("spf_efficiency_bleu_3_se.npy", bleu_3_efficiency_se)
-
-    # d_sr_np = np.array(args.gamma_list) * args.d
-    #
-    # plt.figure()
-    # plt.plot(d_sr_np, mean_semantic_sim)
-    # plt.grid()
-    # plt.xlabel("S-R Distance")
-    # plt.ylabel("Semantic Similarity")
-    # plt.title("Semantic Similarity v. S-R Distance Ratio")
-    # plt.savefig("SemanticSimilarty_v_distance.png", dpi=400)
-    #
-    # plt.figure()
-    # plt.plot(d_sr_np, mean_bleu_1)
-    # plt.grid()
-    # plt.xlabel("S-R Distance")
-    # plt.ylabel("BLEU 1-gram")
-    # plt.title("BLEU 1-gram v. S-R Distance")
-    # plt.savefig("BLEU1gram_v_distance.png", dpi=400)
-    #
-    # plt.figure()
-    # plt.plot(d_sr_np, mean_bleu_2)
-    # plt.grid()
-    # plt.xlabel("S-R Distance")
-    # plt.ylabel("BLEU 2-gram")
-    # plt.title("BLEU 2-gram v. S-R Distance")
-    # plt.savefig("BLEU2gam_v_distance.png", dpi=400)
-    #
-    # plt.figure()
-    # plt.plot(d_sr_np, mean_bleu_3)
-    # plt.grid()
-    # plt.xlabel("S-R Distance")
-    # plt.ylabel("BLEU 3-gram")
-    # plt.title("BLEU 3-gram v. S-R Distance")
-    # plt.savefig("BLEU3gram_v_distance.png", dpi=400)
-    #
-    # plt.figure()
-    # plt.plot(d_sr_np, mean_bleu_4)
-    # plt.grid()
-    # plt.xlabel("S-R Distance")
-    # plt.ylabel("BLEU 4-gram")
-    # plt.title("BLEU 4-gram v. S-R Distance")
-    # plt.savefig("BLEU4gram_v_distance.png", dpi=400)
Index: semantic_communication/models/semantic_transformer.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing import Optional, List\n\nimport torch\nfrom torch import nn\n\nfrom semantic_communication.models.semantic_decoder import SemanticDecoder\nfrom semantic_communication.models.semantic_encoder import SemanticEncoder\nfrom semantic_communication.utils.general import shift_inputs\n\n\nclass SemanticTransformer(nn.Module):\n    def __init__(\n        self,\n        semantic_encoder: SemanticEncoder,\n        semantic_decoder: SemanticDecoder,\n    ):\n        super().__init__()\n        self.semantic_encoder = semantic_encoder\n        self.semantic_decoder = semantic_decoder\n        self.mode = semantic_encoder.mode\n\n    def forward(\n        self,\n        messages: Optional[List[str]] = None,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        snr_db: Optional[float] = None,\n    ):\n        encoder_output = self.semantic_encoder(\n            messages=messages,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n        )\n        encoder_output = self._add_noise(encoder_output, snr_db)\n\n        decoder_idx, targets, enc_padding_mask, is_causal = shift_inputs(\n            xb=input_ids,\n            attention_mask=attention_mask,\n            mode=self.mode,\n        )\n\n        logits, loss = self.semantic_decoder(\n            idx=decoder_idx,\n            encoder_output=encoder_output,\n            is_causal=is_causal,\n            enc_padding_mask=enc_padding_mask,\n            targets=targets,\n        )\n\n        return logits, loss\n\n    def generate(\n        self,\n        messages: Optional[List[str]] = None,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        snr_db: Optional[float] = None,\n        beam_width=5,\n        max_length=20,\n    ):\n        self.eval()\n        with torch.no_grad():\n            encoder_output = self.semantic_encoder(\n                messages=messages,\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n            )\n            encoder_output = self._add_noise(encoder_output, snr_db)\n\n            return self.semantic_decoder.generate(\n                encoder_output=encoder_output,\n                beam_width=beam_width,\n                max_length=max_length,\n            )\n\n    @staticmethod\n    def _add_noise(signal, snr_db):\n        if snr_db is not None:\n            signal_pow = torch.mean(torch.pow(signal, 2), dim=-1, keepdim=True)\n            noise_pow = signal_pow / (10 ** (snr_db / 10))\n\n            noise = torch.sqrt(noise_pow) * torch.randn(\n                size=signal.shape, device=signal.device\n            )\n            return signal + noise\n\n        else:\n            return signal\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/semantic_communication/models/semantic_transformer.py b/semantic_communication/models/semantic_transformer.py
--- a/semantic_communication/models/semantic_transformer.py	(revision cad0f871753e4e48d32315ebcb817d0c732a67c2)
+++ b/semantic_communication/models/semantic_transformer.py	(date 1714336646954)
@@ -57,6 +57,7 @@
         snr_db: Optional[float] = None,
         beam_width=5,
         max_length=20,
+        n_generated_tokens=20,
     ):
         self.eval()
         with torch.no_grad():
@@ -69,8 +70,11 @@
 
             return self.semantic_decoder.generate(
                 encoder_output=encoder_output,
-                beam_width=beam_width,
+                is_causal=False,
                 max_length=max_length,
+                enc_padding_mask=None,
+                beam_width=beam_width,
+                n_generated_tokens=n_generated_tokens,
             )
 
     @staticmethod
Index: semantic_communication/models/semantic_decoder.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom semantic_communication.utils.general import get_device\n\n\nclass MultiInputSequential(nn.Sequential):\n    def forward(self, *inputs):\n        for module in self._modules.values():\n            if type(inputs) == tuple:\n                inputs = module(*inputs)\n            else:\n                inputs = module(inputs)\n        return inputs\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, n_heads, n_embeddings, block_size):\n        super().__init__()\n        self.device = get_device()\n        self.sa_heads = nn.MultiheadAttention(\n            embed_dim=n_embeddings,\n            num_heads=n_heads,\n            dropout=0.1,\n            batch_first=True,\n        )\n        self.ca_heads = nn.MultiheadAttention(\n            embed_dim=n_embeddings,\n            num_heads=n_heads,\n            dropout=0.1,\n            batch_first=True,\n        )\n\n        self.ff_net = nn.Sequential(\n            nn.Linear(n_embeddings, 4 * n_embeddings),\n            nn.ReLU(),\n            nn.Linear(4 * n_embeddings, n_embeddings),  # projection\n            nn.Dropout(0.1),\n        )\n        self.ln1 = nn.LayerNorm(n_embeddings)\n        self.ln2 = nn.LayerNorm(n_embeddings)\n        self.ln3 = nn.LayerNorm(n_embeddings)\n\n        ones_tensor = torch.ones(block_size, block_size, device=self.device)\n        self.register_buffer(\"tril\", torch.tril(ones_tensor, -1).T.bool())\n\n    def forward(\n        self, x, encoder_output, source_padding_mask, enc_padding_mask, is_causal\n    ):\n        # norm before the layer, residual connection after the layer\n        x_normed = self.ln1(x)\n        attention_out = self.sa_heads(\n            query=x_normed,\n            key=x_normed,\n            value=x_normed,\n            key_padding_mask=source_padding_mask,\n            need_weights=False,\n            attn_mask=self.tril,\n            is_causal=True,\n        )[0]\n        x = x + attention_out\n\n        x_normed = self.ln2(x)\n\n        if is_causal:\n            attention_mask = self.tril\n        else:\n            attention_mask = None\n\n        attention_out = self.ca_heads(\n            query=x_normed,\n            key=encoder_output,\n            value=encoder_output,\n            key_padding_mask=enc_padding_mask,\n            need_weights=False,\n            attn_mask=attention_mask,\n            is_causal=is_causal,\n        )[0]\n        x = x + attention_out\n\n        x = x + self.ff_net(self.ln3(x))\n        return x, encoder_output, source_padding_mask, enc_padding_mask, is_causal\n\n\nclass SemanticDecoder(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        n_blocks,\n        n_heads,\n        n_embeddings,\n        block_size,\n        bert,\n        pad_idx,\n    ):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embeddings)\n        self.token_embedding_table.weight = bert.embeddings.word_embeddings.weight\n\n        self.position_embedding_table = nn.Embedding(block_size, n_embeddings)\n\n        self.decoder_blocks = MultiInputSequential(\n            *[\n                DecoderBlock(\n                    n_heads=n_heads,\n                    n_embeddings=n_embeddings,\n                    block_size=block_size,\n                )\n                for _ in range(n_blocks)\n            ]\n        )\n        self.ln = nn.LayerNorm(n_embeddings)\n        self.lm_head = nn.Linear(n_embeddings, vocab_size, bias=False)\n        self.lm_head.weight = self.token_embedding_table.weight\n\n        self.device = get_device()\n        self.pad_idx = pad_idx\n\n    def forward(\n        self, idx, encoder_output, is_causal, enc_padding_mask=None, targets=None\n    ):\n        B, T = idx.shape\n\n        token_embeddings = self.token_embedding_table(idx)  # (B,T,C)\n        pos_embeddings = self.position_embedding_table(\n            torch.arange(T, device=self.device)\n        )  # (T,C)\n        x = token_embeddings + pos_embeddings\n\n        source_padding_mask = idx == self.pad_idx\n\n        x, _, _, _, _ = self.decoder_blocks(\n            x, encoder_output, source_padding_mask, enc_padding_mask, is_causal\n        )\n        logits = self.lm_head(self.ln(x))\n\n        if targets is None:\n            loss = None\n        else:\n            logits = logits.reshape(B * T, -1)\n            targets = targets.reshape(B * T)\n            target_mask = targets != self.pad_idx\n\n            loss = F.cross_entropy(logits[target_mask, :], targets[target_mask])\n\n        return logits, loss\n\n    def generate(\n        self,\n        encoder_output,\n        is_causal,\n        max_length,\n        enc_padding_mask=None,\n        beam_width=5,\n        n_generated_tokens=20,\n    ):\n        B = encoder_output.shape[0]\n        T = n_generated_tokens\n\n        with torch.no_grad():\n            Y = self.pad_idx * torch.ones(B, T).to(self.device).long()\n            Y[:, 0] = 1\n\n            next_logits, _ = self(\n                Y[:, :max_length], encoder_output, is_causal, enc_padding_mask\n            )\n            next_logits = next_logits[:, 0, :]\n            vocab_size = next_logits.shape[-1]\n\n            probabilities, next_chars = F.log_softmax(next_logits, dim=-1).topk(\n                k=beam_width, dim=-1\n            )\n\n            Y = Y.repeat((beam_width, 1))\n            Y[:, 1] = next_chars.flatten()\n\n            for i in range(1, T - 1):\n                start_idx = max(i - max_length, 0)\n                end_idx = start_idx + max_length\n                dataset = TensorDataset(Y[:, -start_idx:end_idx])\n                dl = DataLoader(dataset, batch_size=B)\n                next_probabilities = []\n\n                for x in dl:\n                    next_logits, _ = self(\n                        x[0], encoder_output, is_causal, enc_padding_mask\n                    )\n                    next_logits = next_logits[:, i, :]\n                    next_probabilities.append(F.log_softmax(next_logits, dim=-1))\n\n                next_probabilities = torch.cat(next_probabilities, axis=0)\n                next_probabilities = next_probabilities.reshape(\n                    (-1, beam_width, next_probabilities.shape[-1])\n                )\n                probabilities = probabilities.unsqueeze(-1) + next_probabilities\n                probabilities = probabilities.flatten(start_dim=1)\n                probabilities, idx = probabilities.topk(k=beam_width, axis=-1)\n                next_chars = torch.remainder(idx, vocab_size).flatten().unsqueeze(-1)\n\n                best_candidates = (idx / vocab_size).long()\n                best_candidates += (\n                    torch.arange(\n                        Y.shape[0] // beam_width, device=self.device\n                    ).unsqueeze(-1)\n                    * beam_width\n                )\n\n                Y = Y[best_candidates].flatten(end_dim=-2)\n                Y[:, i + 1] = next_chars.flatten()\n\n                if torch.all(torch.any(Y == 2, dim=1)):\n                    break\n\n            best_indices = torch.argmax(probabilities, dim=1)\n            Y = torch.gather(\n                Y.reshape(-1, beam_width, Y.shape[-1]),\n                1,\n                best_indices.reshape(-1, 1, 1).repeat((1, 1, Y.shape[-1])),\n            ).squeeze(1)\n\n            return Y, probabilities[torch.arange(B), best_indices]\n\n    def generate_next(\n        self,\n        idx,\n        encoder_output,\n        attention_mask=None,\n        sample=False,\n    ):\n        B, T, C = encoder_output.shape\n\n        # get the predictions\n        logits, _ = self(idx, encoder_output, attention_mask)  # (B, T, C)\n        # apply softmax to get probabilities\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n\n        if sample:\n            idx_next = torch.multinomial(\n                probs.view(B * self.block_size, -1),\n                num_samples=1,\n            )\n            idx_next = idx_next.reshape(B, -1)\n        else:\n            idx_next = torch.argmax(probs, dim=-1)\n\n        return idx_next  # (B, T)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/semantic_communication/models/semantic_decoder.py b/semantic_communication/models/semantic_decoder.py
--- a/semantic_communication/models/semantic_decoder.py	(revision cad0f871753e4e48d32315ebcb817d0c732a67c2)
+++ b/semantic_communication/models/semantic_decoder.py	(date 1714336642710)
@@ -147,6 +147,7 @@
 
         return logits, loss
 
+    @torch.no_grad()
     def generate(
         self,
         encoder_output,
@@ -159,68 +160,65 @@
         B = encoder_output.shape[0]
         T = n_generated_tokens
 
-        with torch.no_grad():
-            Y = self.pad_idx * torch.ones(B, T).to(self.device).long()
-            Y[:, 0] = 1
+        Y = self.pad_idx * torch.ones(B, T).to(self.device).long()
+        Y[:, 0] = 1
 
-            next_logits, _ = self(
-                Y[:, :max_length], encoder_output, is_causal, enc_padding_mask
-            )
-            next_logits = next_logits[:, 0, :]
-            vocab_size = next_logits.shape[-1]
+        next_logits, _ = self(
+            Y[:, :max_length], encoder_output, is_causal, enc_padding_mask
+        )
+        next_logits = next_logits[:, 0, :]
+        vocab_size = next_logits.shape[-1]
 
-            probabilities, next_chars = F.log_softmax(next_logits, dim=-1).topk(
-                k=beam_width, dim=-1
-            )
+        probabilities, next_chars = F.log_softmax(next_logits, dim=-1).topk(
+            k=beam_width, dim=-1
+        )
 
-            Y = Y.repeat((beam_width, 1))
-            Y[:, 1] = next_chars.flatten()
+        Y = Y.repeat_interleave(beam_width, dim=0)
+        encoder_output = encoder_output.repeat_interleave(beam_width, dim=0)
+
+        Y[:, 1] = next_chars.flatten()
 
-            for i in range(1, T - 1):
-                start_idx = max(i - max_length, 0)
-                end_idx = start_idx + max_length
-                dataset = TensorDataset(Y[:, -start_idx:end_idx])
-                dl = DataLoader(dataset, batch_size=B)
-                next_probabilities = []
+        for i in range(1, T - 1):
+            start_idx = max(i - max_length, 0)
+            end_idx = start_idx + max_length
+            dataset = TensorDataset(Y[:, -start_idx:end_idx], encoder_output)
+            dl = DataLoader(dataset, batch_size=B)
+            next_probabilities = []
 
-                for x in dl:
-                    next_logits, _ = self(
-                        x[0], encoder_output, is_causal, enc_padding_mask
-                    )
-                    next_logits = next_logits[:, i, :]
-                    next_probabilities.append(F.log_softmax(next_logits, dim=-1))
+            for x in dl:
+                next_logits, _ = self(x[0], x[1], is_causal, enc_padding_mask)
+                next_logits = next_logits[:, i, :]
+                next_probabilities.append(F.log_softmax(next_logits, dim=-1))
 
-                next_probabilities = torch.cat(next_probabilities, axis=0)
-                next_probabilities = next_probabilities.reshape(
-                    (-1, beam_width, next_probabilities.shape[-1])
-                )
-                probabilities = probabilities.unsqueeze(-1) + next_probabilities
-                probabilities = probabilities.flatten(start_dim=1)
-                probabilities, idx = probabilities.topk(k=beam_width, axis=-1)
-                next_chars = torch.remainder(idx, vocab_size).flatten().unsqueeze(-1)
+            next_probabilities = torch.cat(next_probabilities, axis=0)
+            next_probabilities = next_probabilities.reshape(
+                (-1, beam_width, next_probabilities.shape[-1])
+            )
+            probabilities = probabilities.unsqueeze(-1) + next_probabilities
+            probabilities = probabilities.flatten(start_dim=1)
+            probabilities, idx = probabilities.topk(k=beam_width, axis=-1)
+            next_chars = torch.remainder(idx, vocab_size).flatten().unsqueeze(-1)
 
-                best_candidates = (idx / vocab_size).long()
-                best_candidates += (
-                    torch.arange(
-                        Y.shape[0] // beam_width, device=self.device
-                    ).unsqueeze(-1)
-                    * beam_width
-                )
+            best_candidates = (idx / vocab_size).long()
+            best_candidates += (
+                torch.arange(Y.shape[0] // beam_width, device=self.device).unsqueeze(-1)
+                * beam_width
+            )
 
-                Y = Y[best_candidates].flatten(end_dim=-2)
-                Y[:, i + 1] = next_chars.flatten()
+            Y = Y[best_candidates].flatten(end_dim=-2)
+            Y[:, i + 1] = next_chars.flatten()
 
-                if torch.all(torch.any(Y == 2, dim=1)):
-                    break
+            if torch.all(torch.any(Y == 2, dim=1)):
+                break
 
-            best_indices = torch.argmax(probabilities, dim=1)
-            Y = torch.gather(
-                Y.reshape(-1, beam_width, Y.shape[-1]),
-                1,
-                best_indices.reshape(-1, 1, 1).repeat((1, 1, Y.shape[-1])),
-            ).squeeze(1)
+        best_indices = torch.argmax(probabilities, dim=1)
+        Y = torch.gather(
+            Y.reshape(-1, beam_width, Y.shape[-1]),
+            1,
+            best_indices.reshape(-1, 1, 1).repeat((1, 1, Y.shape[-1])),
+        ).squeeze(1)
 
-            return Y, probabilities[torch.arange(B), best_indices]
+        return Y, probabilities[torch.arange(B), best_indices]
 
     def generate_next(
         self,
