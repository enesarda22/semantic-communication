Index: semantic_communication/data_processing/data_handler.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import csv\nfrom typing import List\n\nimport nltk\nimport torch\nfrom w3lib.html import replace_tags\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import (\n    TensorDataset,\n    RandomSampler,\n    DataLoader,\n    SequentialSampler,\n)\n\nfrom semantic_communication.models.semantic_encoder import SemanticEncoder\nfrom semantic_communication.utils.general import RANDOM_STATE\n\n\nclass DataHandler:\n    data_filename = \"IMDB Dataset.csv\"\n\n    def __init__(\n        self,\n        semantic_encoder: SemanticEncoder,\n        batch_size: int,\n        n_samples: int,\n        train_size: float,\n        val_size: float,\n    ):\n        self.semantic_encoder = semantic_encoder\n\n        self.vocab_size = None\n        self.encoder = None\n        self.train_dataloader = None\n        self.val_dataloader = None\n        self.test_dataloader = None\n\n        self.batch_size = batch_size\n        self.n_samples = n_samples\n        self.train_size = train_size\n        self.val_size = val_size\n\n    def load_data(self):\n        messages = self.load_text()\n        messages = self.preprocess_text(messages)\n\n        tokens = self.semantic_encoder.tokenize(messages=messages)\n\n        self.encoder = LabelEncoder()\n        self.encoder.fit(tokens[\"input_ids\"].flatten().to(\"cpu\"))\n        self.vocab_size = len(self.encoder.classes_)\n\n        input_ids = self.encode_tokens(tokens[\"input_ids\"].flatten().to(\"cpu\"))\n        attention_mask = tokens[\"attention_mask\"]\n\n        # First\n        (\n            train_input_ids,\n            test_input_ids,\n            train_attention_mask,\n            test_attention_mask,\n        ) = train_test_split(\n            input_ids,\n            attention_mask,\n            train_size=self.train_size,\n            random_state=RANDOM_STATE,\n        )\n\n        # Second\n        (\n            train_input_ids,\n            val_input_ids,\n            train_attention_mask,\n            val_attention_mask,\n        ) = train_test_split(\n            train_input_ids,\n            train_attention_mask,\n            train_size=(1 - self.val_size),\n            random_state=RANDOM_STATE,\n        )\n\n        train_data = TensorDataset(train_input_ids, train_attention_mask)\n        train_sampler = RandomSampler(train_data)\n        self.train_dataloader = DataLoader(\n            train_data, sampler=train_sampler, batch_size=self.batch_size\n        )\n\n        val_data = TensorDataset(val_input_ids, val_attention_mask)\n        val_sampler = SequentialSampler(val_data)\n        self.val_dataloader = DataLoader(\n            val_data, sampler=val_sampler, batch_size=self.batch_size\n        )\n\n        test_data = TensorDataset(test_input_ids, test_attention_mask)\n        test_sampler = SequentialSampler(test_data)\n        self.test_dataloader = DataLoader(\n            test_data, sampler=test_sampler, batch_size=self.batch_size\n        )\n\n    def load_text(self) -> List[str]:\n        with open(self.data_filename, mode=\"r\", encoding=\"utf-8\") as f:\n            text = [next(csv.reader(f))[0] for _ in range(self.n_samples + 1)]\n\n        text = text[1:]  # first line is the columns\n        return text\n\n    @staticmethod\n    def preprocess_text(text: List[str]) -> List[str]:\n        sentences_list = [\n            nltk.sent_tokenize(replace_tags(m, \" \")) for m in text\n        ]\n        sentences = sum(sentences_list, [])\n        return sentences\n\n    def get_tokens(\n        self,\n        ids,\n        attention_mask=None,\n        skip_special_tokens=False,\n    ) -> List[str]:\n        if attention_mask is not None:\n            pad_token_id = self.encoder.transform([0])[0]\n            ids = torch.masked_fill(ids, attention_mask == 0, pad_token_id)\n\n        token_ids = self.encoder.inverse_transform(ids.flatten().to(\"cpu\"))\n        token_ids = token_ids.reshape(ids.shape)\n\n        tokens = [\n            self.semantic_encoder.tokenizer.decode(\n                t, skip_special_tokens=skip_special_tokens\n            )\n            for t in token_ids\n        ]\n        return tokens\n\n    def get_text(self, ids):\n        token_list = self.get_tokens(ids)\n        return (\n            ((\" \".join(token_list)).replace(\"[PAD]\", \"\"))\n            .replace(\"[SEP]\", \"\")\n            .strip()\n        )\n\n    def encode_tokens(self, tokens):\n        ids = self.encoder.transform(tokens)\n        ids = ids.reshape(-1, self.semantic_encoder.max_length)\n        return torch.LongTensor(ids)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/semantic_communication/data_processing/data_handler.py b/semantic_communication/data_processing/data_handler.py
--- a/semantic_communication/data_processing/data_handler.py	(revision d40497c612386c1fa023a5f6997279cd58fb2f43)
+++ b/semantic_communication/data_processing/data_handler.py	(date 1698718101844)
@@ -135,14 +135,6 @@
         ]
         return tokens
 
-    def get_text(self, ids):
-        token_list = self.get_tokens(ids)
-        return (
-            ((" ".join(token_list)).replace("[PAD]", ""))
-            .replace("[SEP]", "")
-            .strip()
-        )
-
     def encode_tokens(self, tokens):
         ids = self.encoder.transform(tokens)
         ids = ids.reshape(-1, self.semantic_encoder.max_length)
Index: semantic_communication/models/transceiver.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\nimport torch\nfrom torch import nn\n\nfrom semantic_communication.models.semantic_decoder import SemanticDecoder\nfrom semantic_communication.models.semantic_encoder import SemanticEncoder\nfrom semantic_communication.utils.channel import Channel\nfrom semantic_communication.utils.general import get_device\n\n\nclass ChannelEncComp(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(ChannelEncComp, self).__init__()\n        self.linear = nn.Linear(in_dim, out_dim)\n        self.bn = nn.BatchNorm1d(out_dim)\n        self.prelu = nn.PReLU()\n\n    def forward(self, x):\n        x = self.linear(x).transpose(1, 2)\n        x = self.bn(x).transpose(1, 2)\n\n        out = self.prelu(x)\n        return out\n\n\nclass ChannelEncoder(nn.Module):\n    def __init__(self, nin, nout):\n        super(ChannelEncoder, self).__init__()\n        up_dim = int(np.floor(np.log2(nin) / 2))\n        low_dim = int(np.ceil(np.log2(nout) / 2))\n\n        dims = [nin]\n        for i in range(up_dim - low_dim + 1):\n            dims.append(np.power(4, up_dim - i))\n\n        self.layers = nn.ModuleList(\n            [\n                ChannelEncComp(dims[i], dims[i + 1])\n                for i in range(len(dims) - 1)\n            ]\n        )\n\n        self.linear = nn.Linear(dims[-1], nout)\n\n    def forward(self, x):\n        for l in self.layers:\n            x = l(x)\n        return self.linear(x)\n\n\nclass ChannelDecoder(nn.Module):\n    # construct the model\n\n    def __init__(self, nin, nout):\n        super(ChannelDecoder, self).__init__()\n        up_dim = int(np.floor(np.log2(nout) / 2))\n        low_dim = int(np.ceil(np.log2(nin) / 2))\n        dims = [nin]\n        for i in range(up_dim - low_dim + 1):\n            dims.append(np.power(4, low_dim + i))\n\n        self.layers = nn.ModuleList(\n            [\n                ChannelEncComp(dims[i], dims[i + 1])\n                for i in range(len(dims) - 1)\n            ]\n        )\n\n        self.linear = nn.Linear(dims[-1], nout)\n\n    def forward(self, x):\n        for l in self.layers:\n            x = l(x)\n        return self.linear(x)\n\n\nclass TxRelayChannelModel(nn.Module):\n    def __init__(self, nin, n_latent, channel: Channel):\n        super(TxRelayChannelModel, self).__init__()\n\n        self.tx_encoder = ChannelEncoder(nin, n_latent)\n        self.relay_decoder = ChannelDecoder(n_latent, nin)\n        self.channel = channel\n\n    def forward(self, x):\n        ch_input = self.tx_encoder(x)\n        ch_output = self.channel(ch_input)\n        x_hat = self.relay_decoder(ch_output)\n        return x_hat\n\n\nclass TxRelayRxChannelModel(nn.Module):\n    def __init__(\n        self,\n        nin,\n        n_latent,\n        channel_tx_rx: Channel,\n        channel_rel_rx: Channel,\n    ):\n        super(TxRelayRxChannelModel, self).__init__()\n\n        self.tx_encoder = ChannelEncoder(nin, n_latent)\n        self.relay_encoder = ChannelEncoder(nin, n_latent)\n        self.rx_decoder = ChannelDecoder(n_latent, nin)\n        self.channel_tx_rx = channel_tx_rx\n        self.channel_rel_rx = channel_rel_rx\n\n    def forward(self, tx_x, rel_x):\n        tx_ch_input = self.tx_encoder(tx_x)\n        rel_ch_input = self.relay_encoder(rel_x)\n\n        # Superpose\n        ch_output = self.channel_tx_rx(tx_ch_input) + self.channel_rel_rx(\n            rel_ch_input\n        )\n        x_hat = self.rx_decoder(ch_output)\n        return x_hat  # ground truth = tx_x + rel_x\n\n\nclass Transceiver(nn.Module):  # TODO: find a cooler name\n    def __init__(\n        self,\n        semantic_encoder: SemanticEncoder,\n        relay_semantic_decoder: SemanticDecoder,\n        rx_semantic_decoder: SemanticDecoder,\n        tx_relay_channel_enc_dec: TxRelayChannelModel,\n        tx_relay_rx_channel_enc_dec: TxRelayRxChannelModel,\n    ):\n        super().__init__()\n        self.tx_semantic_encoder = semantic_encoder\n        self.relay = Relay(semantic_encoder, relay_semantic_decoder)\n        self.rx_semantic_decoder = rx_semantic_decoder\n\n        self.tx_relay_channel_enc_dec = tx_relay_channel_enc_dec\n        self.tx_relay_rx_channel_enc_dec = tx_relay_rx_channel_enc_dec\n\n    def forward(self, w, attention_mask):\n        # transmitter\n        x = self.tx_semantic_encoder(\n            input_ids=w,\n            attention_mask=attention_mask,\n        )\n\n        # relay\n        x_hat = self.tx_relay_channel_enc_dec(x[:, :-1, :])\n        x_relay = self.relay(x_hat)\n\n        # receiver\n        x_hat_rcv = self.tx_relay_rx_channel_enc_dec(x[:, 1:, :], x_relay)\n        s_hat = self.rx_semantic_decoder(\n            encoder_output=x_hat_rcv,\n            attention_mask=attention_mask[:, 1:],\n            targets=w[:, 1:],\n        )\n        return s_hat\n\n\nclass Relay(nn.Module):\n    def __init__(\n        self,\n        semantic_encoder: SemanticEncoder,\n        semantic_decoder: SemanticDecoder,\n    ):\n        super().__init__()\n        self.semantic_encoder = semantic_encoder\n        self.semantic_decoder = semantic_decoder\n\n    def forward(self, x):\n        B, T, C = x.shape\n\n        self.semantic_decoder.eval()\n        with torch.no_grad():\n            predicted_ids = self.semantic_decoder.generate(x)\n\n        # ids are repeated to generate the embeddings sequentially\n        predicted_ids = torch.repeat_interleave(predicted_ids, T, dim=0)\n\n        # append [CLS] token\n        cls_padding = torch.full((B * T, 1), 2).to(get_device())\n        predicted_ids = torch.cat(\n            tensors=(cls_padding, predicted_ids),\n            dim=1,\n        )\n\n        # tril mask to generate the embeddings sequentially\n        tril_mask = torch.tril(\n            torch.ones(T, T + 1, dtype=torch.long),\n            diagonal=1,\n        ).repeat(B, 1)\n\n        out = self.semantic_encoder(\n            input_ids=predicted_ids,\n            attention_mask=tril_mask,\n        )\n\n        # use eye mask to select the correct embeddings sequentially\n        eye_mask = torch.eye(T).repeat(1, B) == 1\n        out = torch.masked_select(out[:, 1:, :].transpose(-1, 0), eye_mask)\n        out = out.view(B, T, C)\n\n        return out\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/semantic_communication/models/transceiver.py b/semantic_communication/models/transceiver.py
--- a/semantic_communication/models/transceiver.py	(revision d40497c612386c1fa023a5f6997279cd58fb2f43)
+++ b/semantic_communication/models/transceiver.py	(date 1698786640164)
@@ -192,7 +192,7 @@
             input_ids=predicted_ids,
             attention_mask=tril_mask,
         )
-
+        # TODO: should relay decoder mask after end token prediction?
         # use eye mask to select the correct embeddings sequentially
         eye_mask = torch.eye(T).repeat(1, B) == 1
         out = torch.masked_select(out[:, 1:, :].transpose(-1, 0), eye_mask)
