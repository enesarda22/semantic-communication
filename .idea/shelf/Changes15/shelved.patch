Index: eval_codec.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/eval_codec.py b/eval_codec.py
new file mode 100644
--- /dev/null	(date 1704911687178)
+++ b/eval_codec.py	(date 1704911687178)
@@ -0,0 +1,126 @@
+import argparse
+
+import torch
+from torch.nn import functional as F
+from tqdm import tqdm
+
+from semantic_communication.data_processing.data_handler import DataHandler
+from semantic_communication.models.codec import Codec
+from semantic_communication.models.semantic_decoder import SemanticDecoder
+from semantic_communication.models.semantic_encoder import SemanticEncoder
+from semantic_communication.models.transceiver import ChannelEncoder
+from semantic_communication.utils.general import (
+    add_semantic_decoder_args,
+    add_data_args,
+    set_seed,
+    get_device,
+    load_model,
+    shift_inputs,
+)
+
+if __name__ == "__main__":
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--codec-path", type=str)
+    parser.add_argument("--source_code_dim", type=int)
+    parser.add_argument("--batch-size", default=128, type=int)
+    parser.add_argument("--n-batches", default=100, type=int)
+
+    add_semantic_decoder_args(parser)
+    add_data_args(parser)
+
+    args = parser.parse_args()
+    device = get_device()
+    set_seed()
+
+    semantic_encoder = SemanticEncoder(max_length=args.max_length)
+    data_handler = DataHandler(
+        semantic_encoder=semantic_encoder,
+        data_fp=args.data_fp,
+        batch_size=args.batch_size,
+    )
+
+    encoder = ChannelEncoder(nin=args.n_embeddings, nout=args.source_code_dim)
+    semantic_decoder = SemanticDecoder(
+        vocab_size=data_handler.vocab_size,
+        n_blocks=args.n_blocks,
+        n_heads=args.n_heads,
+        n_embeddings=args.n_embeddings,
+        block_size=args.max_length,
+        semantic_encoder=semantic_encoder,
+        label_encoder=data_handler.label_encoder,
+    ).to(device)
+
+    codec = Codec(
+        encoder=encoder,
+        decoder=semantic_decoder,
+        encoder_out_dim=args.source_code_dim,
+        embedding_dim=args.n_embeddings,
+    )
+    load_model(codec, args.codec_path)
+
+    mse = 0
+    codec.eval()
+    for i, b in tqdm(enumerate(data_handler.val_dataloader)):
+        xb = b[0].to(device)
+        B, T = xb.shape
+        attention_mask = b[1].to(device)
+
+        encoder_output_full = semantic_encoder(
+            input_ids=xb,
+            attention_mask=attention_mask,
+        )
+
+        xb = data_handler.label_encoder.transform(xb)
+        idx, encoder_output, attention_mask, targets = shift_inputs(
+            xb=xb,
+            encoder_output=encoder_output_full,
+            attention_mask=attention_mask,
+            mode=args.mode,
+        )
+
+        with torch.no_grad():
+            logits, _ = codec(
+                idx=idx,
+                x=encoder_output,
+                attention_mask=attention_mask,
+                targets=targets,
+            )
+
+        probs = F.softmax(logits, dim=-1)
+        predicted_ids = (torch.argmax(probs, dim=-1)).reshape(B, args.max_length)
+
+        end_token_id = data_handler.label_encoder.transform(torch.LongTensor([102]))[
+            0
+        ].item()
+        end_prediction_idx = torch.argmax(
+            predicted_ids.eq(end_token_id).double(), dim=1
+        )
+
+        # zero means no end token prediction
+        end_prediction_idx[end_prediction_idx == 0] = T - 1
+
+        # prediction mask is created based on end token predictions
+        pred_mask = (torch.arange(T - 1).to(device)).le(end_prediction_idx.view(-1, 1))
+
+        predicted_sentences = data_handler.get_tokens(
+            ids=predicted_ids,
+            attention_mask=pred_mask,
+            skip_special_tokens=True,
+        )
+        original_sentences = data_handler.get_tokens(
+            ids=targets,
+            attention_mask=attention_mask,
+            skip_special_tokens=True,
+        )
+
+        predicted_embeddings = semantic_encoder(messages=predicted_sentences)
+
+        mse += torch.sum(
+            (predicted_embeddings[:, 0, :] - encoder_output_full[:, 0, :]) ** 2
+        ).item()
+
+        if i > args.n_batches:
+            break
+
+    mse = mse / (args.n_batches * args.batch_size)
+    print(f"MSE: {mse:.3f}")
Index: semantic_communication/models/codec.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/semantic_communication/models/codec.py b/semantic_communication/models/codec.py
new file mode 100644
--- /dev/null	(date 1704911687188)
+++ b/semantic_communication/models/codec.py	(date 1704911687188)
@@ -0,0 +1,24 @@
+from torch import nn
+
+from semantic_communication.models.semantic_decoder import SemanticDecoder
+from semantic_communication.models.transceiver import ChannelEncoder
+
+
+class Codec(nn.Module):
+    def __init__(
+        self,
+        encoder: ChannelEncoder,
+        decoder: SemanticDecoder,
+        encoder_out_dim: int,
+        embedding_dim: int,
+    ):
+        super().__init__()
+        self.encoder = encoder
+        self.mapping_layer = nn.Linear(encoder_out_dim, embedding_dim)
+        self.decoder = decoder
+
+    def forward(self, idx, x, attention_mask=None, targets=None):
+        x = self.encoder(x)
+        x = self.mapping_layer(x)
+        logits, loss = self.decoder(idx, x, attention_mask, targets)
+        return logits, loss
